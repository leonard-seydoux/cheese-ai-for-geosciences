{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9669f0c9-4852-4b0c-80b6-5de988a28536",
   "metadata": {},
   "source": [
    "# \"Physics-based\" learning from equations: PINNs\n",
    "\n",
    "This notebook is an introduction to regression problems that are found when dealing with partial differential equations (PDE) derived for (geo)physical systems. In particular, we will explore a general method that informs the machine learning model about the underlying governing equations we are trying to solve.\n",
    "\n",
    "## 1. Introduction\n",
    "### 1.1 Objectives\n",
    "\n",
    "**The goal of this notebook is to learn about physics-informed approaches and how they are implemented in practice. The student will be encouraged in particular, to play with the physics-based loss function to solve different sets of equations.**\n",
    "\n",
    "Physics-Informed Neural Networks (abbreviated PINNs) were first introduced in 2019 by [Raissi et al.](https://doi.org/10.1016/j.jcp.2018.10.045) as a \"data-efficient universal function approximators that naturally encode any underlying physical laws as prior information\". First, this means that PINNs can be used as drop-in remplacement to classical numerical solvers such as finite differences, finite volumes or spectral methods. Then, the concept of PINNs is also built on the important universal approximation theorem, which states that there always exists an architecture that approximates the target function to a certain precision. Finally, the success of PINNs also comes from their simplicity and the low (or even non-existent) data requirements. To be more precise, a PINN with parameters $\\lambda$ can predict non-linear partial differential operators $\\mathcal{N}(u; \\lambda)$ that solves equations of the form\n",
    "\n",
    "$$ u_{t} + \\mathcal{N}(u; \\lambda) = 0 $$\n",
    "\n",
    "### 1.2. Differentiable programming (or automatic differentiation)\n",
    "\n",
    "The algorithms explored in this notebook can be implemented with any differentiable programming framework, i.e., any library that offers automatic differentiation capabilities. Automatic differentiation can be used to evaluate any partial derivative\n",
    "\n",
    "$$ \\frac{\\partial u}{\\partial x}\\biggr|_v $$\n",
    "\n",
    "numerically at $v$. It differs from symbolic differentiation in that it does not give a mathematical expression but a numerical value. It also differs from numerical differentiation via e.g., finite differences in that it evaluates the exact derivative and not an approximation.\n",
    "\n",
    "In fact, automatic differentiation is the building block of backpropagation, the algorithm used by any machine learning framework which estimates the gradient (or sensitivity) of the neural network with respect to its trainable parameters.\n",
    "\n",
    "### 1.3. Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb8f21-ff9c-43cb-b773-3d94ea974f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02726379-6a3f-4c43-a228-e4ba687bd4cd",
   "metadata": {},
   "source": [
    "> *Note*: We will use the Python library JAX for this tutorial. It has a similar interface than NumPy, i.e. you call `jnp.` instead of `np.`, but the functions can run on any hardware (CPUs, GPUs and TPUs). JAX also offers some program transformation capabilities that are described with some experiments in Supplement 1 (S1) at the end of this notebook.\n",
    "\n",
    "## 2. Physics-Informed Neural Networks\n",
    "\n",
    "### 2.1 A priori known information about the PDE: initial and boundary conditions\n",
    "\n",
    "Recall that when training a PINN, we need to constrain the learning with known information about the PDE. These constraints come as individual loss functions:\n",
    "* $L_{\\mathrm{eq}}$ is the loss on the equation residual.\n",
    "* $L_{\\mathrm{IC}}$ is the loss on the initial condition.\n",
    "* $L_{\\mathrm{BC}}$ is the loss on the boundary conditions.\n",
    "\n",
    "and the total loss $L$ used to optimize the model takes these three into account:\n",
    "\n",
    "$$ L = L_{\\mathrm{eq}} + L_{\\mathrm{IC}} + L_{\\mathrm{BC}}. $$\n",
    "\n",
    "As a demonstration, we will start with a one-dimensional [Burgers' equation](https://en.wikipedia.org/wiki/Burgers%27_equation), a non-linear time-dependent PDE. Burgers' equation reads\n",
    "\n",
    "$$ \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2} \\quad  \\forall (t,x) \\in \\mathcal{T} \\times \\mathcal{X}, $$\n",
    "\n",
    "where $\\nu$ is the diffusivity, $\\mathcal{T}$ is the time domain $[0,T]$ and $\\mathcal{X}$ is the 1D spatial domain to be defined below. \n",
    "The goal of the PINN here will be to produce an approximation of $u$ given time $t$ and spatial position $x$. We will train a neural network (model) $\\mathrm{NN}$ such that $\\mathrm{NN}(t, x) \\approx u(t, x)$.\n",
    "\n",
    "**2.1.1. Initial conditions**\n",
    "\n",
    "Let us start with the initial condition,\n",
    "\n",
    "$$ u(t=0, x) = f(x). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee92aa6-cd43-44b4-9394-604d965e66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ic(x):\n",
    "  \"\"\" Initial conditions of the system. \"\"\"\n",
    "  return jnp.exp(-x**2/2)\n",
    "\n",
    "def ic_mis(upred_t0, x):\n",
    "  \"\"\" Mismatch between predicted initial condition and true initial condition (squared error). \"\"\"\n",
    "  return jnp.square(upred_t0 - ic(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0d0aa-da33-4612-a188-d9a5785ca4cd",
   "metadata": {},
   "source": [
    "**2.1.2. Boundary conditions**\n",
    "\n",
    "Now, we need a way to penalize the boundary conditions. We define the spatial domain  $\\mathcal{X}$ as the open set $]a, b[$. We wish the solution to satisfy Dirichlet boundary conditions (BCs) at both ends of the domain: \n",
    "\n",
    "$$ u(t, x=a) = c_{a}, \\quad u(t, x=b) = c_{b} \\quad \\forall t \\in [0,T].$$\n",
    "\n",
    "In order to penalize the Dirichlet BCs, we can substract the values $u_{\\mathrm{pred}}$ predicted by the model with the expected values at the boundary. Note that for any loss, the reduction for the spatio-temporal domain prediction is done by taking the mean. For the Dirichlet BC loss, if we take the difference of the predicted and exact boundary value, $L_{\\mathrm{BC}}$ will be given as the mean of sampled times $t$ in the defined time domain $\\mathcal{T}$\n",
    "\n",
    "$$ L_{\\mathrm{BC}} = \\frac{1}{\\mathcal{T}} \\sum_{t \\sim \\mathcal{T}} (\\mathrm{NN}(t, a) - c_a)^2 + (\\mathrm{NN}(t, b) - c_b)^2 $$\n",
    "\n",
    "*Remark*: It is important that the initial conditions also fulfil the boundary conditions, otherwise the problem is ill posed and learning might fail or give invalid results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a606ef-03f0-4615-b7a5-203afa425bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates of the left and right boundaries\n",
    "a = -2 * jnp.pi\n",
    "b =  2 * jnp.pi\n",
    "\n",
    "# Values for Dirichlet boundary conditions\n",
    "c_a = 0\n",
    "c_b = 0\n",
    "\n",
    "def bc_mis(upred_a, upred_b):\n",
    "  \"\"\" Mismatch between predicted boundary values and exact values (squared error). \"\"\"\n",
    "  return jnp.square(upred_a - c_a) + jnp.square(upred_b - c_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60519a0-1308-4526-80f8-16604eb008ca",
   "metadata": {},
   "source": [
    "**2.1.3. Equation residual**\n",
    "\n",
    "The final part of the Physics-Informed loss function is the equation residual loss. Here, we use automatic-differentiation capabilities to write the partial derivatives exactly. With `JAX`, we use the function `jax.grad` for a quantity with respect to the argument indicated by the parameter `argnums`. For the partial derivative of $u$ with respect to time $t$, \n",
    "\n",
    "$$ \\frac{\\partial u(t, x)}{\\partial t} $$\n",
    "\n",
    "we write ``dudt = jax.grad(u, argnums=0)`` where `u` is a function that takes `t` and `x` as arguments (**in this specific order**). We can call `jax.grad` multiple times to obtain higher-order derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d8be2-6556-444e-9502-6fd05eec2dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01\n",
    "def eq_res(u_pred, t, x):\n",
    "  \"\"\" Equation residual. Note: u_pred is a callback to predict u(t, x) from using NN. \"\"\"\n",
    "  u = u_pred(t, x)\n",
    "  u_t = jax.grad(u_pred, argnums=0)(t, x)\n",
    "  u_x = jax.grad(u_pred, argnums=1)(t, x)\n",
    "  u_xx = jax.grad(jax.grad(u_pred, argnums=1), argnums=1)(t, x)\n",
    "  f = u_t + u * u_x - nu * u_xx\n",
    "  return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b95b13-e04f-441c-b0cb-a7a2d8229ece",
   "metadata": {},
   "source": [
    "### 2.2 Setting and running the PINN learning problem\n",
    "\n",
    "**2.2.1 Setting** \n",
    "\n",
    "We are now ready to create the PINN. To set the PINN, we need to define the temporal horizon, here chosen to be $t \\in [0, T]$ and the size of the spatio-temporal space we want to explore, i.e., the number of sample points $N_x$ and $N_t$ in the $x$ and $t$ dimensions, respectively.\n",
    "\n",
    "**Training dataset?** When training a PINN, it is not required to create a training dataset explicitly since the \"input\" data can be sampled in time $t \\sim \\mathcal{T}$ and space $x \\sim \\mathcal{X}$, and the \"target\" data is only based on the knowledge of the PDE (initial conditions, boundary conditions and equation residual). In this notebook, the PINN samples $N_t$ and $N_x$ points in $\\mathcal{T}$ and $\\mathcal{X}$ for each epochs, so that the domains can be covered as much as possible as we run new training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44137c52-8054-45e0-9c21-24b0d6701841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of the temporal horizon\n",
    "T = 5.0\n",
    "\n",
    "pinn = models.PhysicsInformed(\n",
    "  N_x=300,\n",
    "  N_t=200,\n",
    "  t_domain=(0, T),\n",
    "  x_domain=(a, b),\n",
    ")\n",
    "\n",
    "# At each epoch, we sample new random values of t and x from the temporal and spatial domains.\n",
    "t, x = pinn.sample_space_time(N_x=100, N_t=100)\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.scatter(t, x, marker='x', c='k')\n",
    "axs.set_xlabel(r'$t$')\n",
    "axs.set_ylabel(r'$x$')\n",
    "fig.suptitle(r'Sampled spatio-temporal domain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ed203-e6d6-4440-aef0-5d3c82a119c3",
   "metadata": {},
   "source": [
    "**2.2.2 Training**\n",
    "\n",
    "To train the model, we need to define a `backend`. The `backend` is the neural architecture $\\mathrm{NN}$ with trainable parameters that will be used to predict $u$. Here, the architecture is required to take two features and return a single prediction, since we have one quantity of interest $u$ and two inputs $t$ and $x$. We have prepared a simple MultiLayer Perceptron (MLP) in the `models.py` file.\n",
    "\n",
    "The `PhysicsInformed.train` function takes the prescribed (and initialized) `backend` and the three physics-informed functions: initial condition mismatch, boundary condition mismatch and equation residual. It also requires the `learning_rate` used during the optimization and the number of training `epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf71ca39-3aeb-4647-b6fc-fc1a774975c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.train(\n",
    "  backend=models.MLP(n_blocks=9, features=20),\n",
    "  ic_fn=ic_mis, # The initial condition mismatch\n",
    "  bc_fn=bc_mis, # The boundary condition mismatch\n",
    "  eq_fn=eq_res, # The equation residual\n",
    "  learning_rate=5e-4,\n",
    "  epochs=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c40f7-9610-4d49-97ed-3acf607aaf0f",
   "metadata": {},
   "source": [
    "## 3. Evaluating\n",
    "\n",
    "### 3.1. PINN: a compact **temporal** discretization\n",
    "\n",
    "We can now explore the solution learned from our PINN. In classical numerical methods to solve differential equations, we need to advance the solution in time from the initial conditions, using for example Runge Kutta integrators. \n",
    "\n",
    "Here, the neural operator does not require any temporal integration scheme, and we can just predict the value of $u$ at any time $t$. Let us predict the solution $u$ at final time $T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcbf330-e03d-48c7-93d8-6cfec075101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True initial condition\n",
    "u_0 = ic(x)\n",
    "\n",
    "# Predict u using the trained PINN at time t=0 and t=T\n",
    "u_nn = pinn.predict([0.0, T], x)\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.scatter(x, u_0, s=5, label=r'$u(t = 0, x)$')\n",
    "axs.scatter(x, u_nn[ 0], s=5, label=r'$\\mathrm{NN}(t = 0, x)$')\n",
    "axs.scatter(x, u_nn[-1], s=5, label=r'$\\mathrm{NN}(t = T, x)$')\n",
    "axs.set_xlabel(r'$x$')\n",
    "axs.set_ylabel(r'$u$')\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d28c891-85cc-4291-8f69-0de4eaecb805",
   "metadata": {},
   "source": [
    "### 3.2. PINN: a compact **spatial** discretization\n",
    "\n",
    "In the previous predictions, we used the $N_x$ sampled spatial points, but again, a PINN can be used to predict the solution $u$ for any type of grid discretization, this allows arbitrary spatial precision of the prediction. \n",
    "\n",
    "Here, we compare the output of the PINN with two predictions, on a coarse grid with 10 points and a finer grid with 500 points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe11fc-8e44-42ac-95ef-b5d3f2d71ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a coarse equispaced grid with 10 points\n",
    "x_coarse = jnp.linspace(a, b, 10)\n",
    "# Define a fine equispaced grid with 500 points\n",
    "x_fine = jnp.linspace(a, b, 500)\n",
    "\n",
    "u_nn_coarse = pinn.predict([0.0, T], x_coarse)\n",
    "u_nn_fine = pinn.predict([0.0, T], x_fine)\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.plot(x_coarse, u_nn_coarse[ 0], label=r'$\\mathrm{NN}(t = 0, x_{\\mathrm{coarse}})$')\n",
    "axs.plot(x_coarse, u_nn_coarse[-1], label=r'$\\mathrm{NN}(t = T, x_{\\mathrm{coarse}})$')\n",
    "axs.plot(x_fine, u_nn_fine[ 0], label=r'$\\mathrm{NN}(t = 0, x_{\\mathrm{fine}})$')\n",
    "axs.plot(x_fine, u_nn_fine[-1], label=r'$\\mathrm{NN}(t = T, x_{\\mathrm{fine}})$')\n",
    "axs.set_xlabel(r'$x$')\n",
    "axs.set_ylabel(r'$u$')\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fd678d-efde-4c6a-9660-922ab7a4a46c",
   "metadata": {},
   "source": [
    "### 3.3. A spatial-temporal neural solver\n",
    "\n",
    "One can combine both predictions from 3.1. and 3.2. to produce a spatial-temporal prediction. Our PINN is a compact solver that approximates the prescribed equation with given initial and boundary conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4dffad-455b-40c7-a020-47ef49fb4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatio-temporal equispaced discretization \n",
    "t_s = np.linspace(0, T, 200)\n",
    "x_s = np.linspace(a, b, 200)\n",
    "\n",
    "u_nn = pinn.predict(t_s, x_s)\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "fig.colorbar(axs.contourf(t_s, x_s, u_nn.T, levels=100, cmap='rainbow'), ax=axs)\n",
    "axs.set_xlabel(r'$t$')\n",
    "axs.set_ylabel(r'$x$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214595e-feee-4f9e-8c5e-099fba4f9907",
   "metadata": {},
   "source": [
    "## 3.4. Quantitative evaluation\n",
    "\n",
    "A precise evaluation of the solver would consist in comparing its predictions with other, well evaluated numerical methods such as finite differences. This is beyond the scope of this tutorial. Instead, it is possible to have an idea of the PINN's precision by looking at the different loss functions. \n",
    "\n",
    "Here, we do not have here a clear separation between training and testing datasets, since training data were generated randomly. However, we can assume that the spatio-temporal discretization above ($N_x = N_t = 200$ equally spaced points) has not been seen during training.\n",
    "\n",
    "Let us evaluate the three loss functions with the previous predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b15a45-aae4-453a-86b7-7156d92b3bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_ic(u_nn_0):\n",
    "  return jnp.mean(ic_mis(u_nn_0, x_s))\n",
    "\n",
    "def L_bc(u_nn_a, u_nn_b):\n",
    "  return jnp.mean(bc_mis(u_nn_a, u_nn_b))\n",
    "\n",
    "def L_eq():\n",
    "  \"\"\" Get the residual by evaluating the equation `eq` at each (t, x) couples. \"\"\"\n",
    "  eq_residual = pinn.residual(t_s, x_s, eq_res)\n",
    "  return jnp.mean(jnp.square(eq_residual))\n",
    "\n",
    "print('Initial condition loss L_ic =',L_ic(u_nn[0]))\n",
    "print('Boundary condition loss L_bc =',L_bc(u_nn[:, 0], u_nn[:, -1]))\n",
    "print('Equation residual loss L_eq =',L_eq())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71a493-5ca2-4a1d-a960-72d0bdc5bf63",
   "metadata": {},
   "source": [
    "We can see that there is room for improvement, in particular for the equation residual. Just as for any numerical method, the precision of the PINN is also controllable. The accuracy of the solution can depend on the backend's number of degrees of freedom (given by the neural architecture), but also the size of the sampled space.\n",
    "\n",
    "## 4. Experimenting\n",
    "\n",
    "In order to experiment with PINNs, we propose the following exercises:\n",
    "\n",
    "---\n",
    "\n",
    "> *Exercise 1*: Try to increase the number of epochs when training the PINN (from 2000 to 6000) and explore the new solutions. What do you observe?\n",
    "\n",
    "> *Exercise 2*: Use the trained PINN to predict a solution beyond the temporal horizon used during training (extrapolation), i.e., at $t > T$. Do you recognize some common limitations of NN?\n",
    "\n",
    "> *Exercise 3*: Experiment with a new set of initial conditions, boundary conditions and equations. For example, try to impose periodic boundary conditions. Also, try to learn a PINN for an other PDE of your choice (e.g. heat equation, wave equation, etc). If the solution is not satisfying, try changing the backend (in `models.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b48b4-5417-4823-908d-f2772987cfa8",
   "metadata": {},
   "source": [
    "### S.1 An introduction to JAX\n",
    "\n",
    "[JAX](https://jax.readthedocs.io/en/latest/) is \"a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning\". Just as PyTorch and TensorFlow, JAX gives you automatic differentiation, via program transformation (**that can be composed**). We choose JAX for this notebook, because it has been designed specifically for scientific machine learning (understand here the application of machine learning to science applications, which is largely interested in differential equations). Remember that except for some small specificities, anything implemented in this notebook is also compatible with PyTorch and TensorFlow.\n",
    "\n",
    "**S.1.1 The JAX-numpy interface**\n",
    "\n",
    "JAX comes with an interface that reproduces as close as possible the one from `numpy` and `scipy`, except that the functions can be run on any hardware (CPU, GPU and TPU) and support different program transformation features. Here, we have imported `jax.numpy as jnp` and we can now use any classical `numpy` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8eb12-fe91-4c7e-bb16-dc34416b06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_numpy = np.zeros(10)\n",
    "print(zeros_numpy)\n",
    "zeros_jax = jnp.zeros(10)\n",
    "print(zeros_jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626da38-5f00-49b6-954f-94fb28ee322a",
   "metadata": {},
   "source": [
    "**S.1.2 Just-in-time compilation**\n",
    "\n",
    "JAX cares about speed. It is built around [XLA](https://github.com/openxla/xla), a compiler based on LLVM that performs many optimization to a given code, so that it runs faster on both CPUs and GPUs. In JAX, to transform a function using on-the-fly, or just-in-time (JIT), compilation, we can just call `jax.jit` on the corresponding function.\n",
    "\n",
    "---\n",
    "\n",
    "> *Exercise 1* : Run the following code section, then try to reduce the size `N` of the array `x`. What do you observe? Can you comment on this behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7698938-b961-4218-8a0c-8f3c301350b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a gaussian using NumPy (note the np.)\n",
    "def gaussian_numpy(x):\n",
    "  return np.exp(-x**2)\n",
    "# Compute a gaussian using JAX (note the jnp.)\n",
    "def gaussian_jax(x):\n",
    "  return jnp.exp(-x**2)\n",
    "\n",
    "N = 1000000\n",
    "\n",
    "x_numpy = np.linspace(-1.0, 1.0, N)\n",
    "print('Gaussian evaluation time with NumPy (CPU): ', end='')\n",
    "%timeit gaussian_numpy(x_numpy)\n",
    "\n",
    "x_jax = jnp.linspace(-1.0, 1.0, N)\n",
    "print('Gaussian evaluation time with JAX (GPU): ', end='')\n",
    "%timeit gaussian_jax(x_jax)\n",
    "\n",
    "# Now, transform the function with JIT compilation\n",
    "gaussian_jax_jit = jax.jit(gaussian_jax)\n",
    "# JIT compilation actually happens during the first call of the function\n",
    "_ = gaussian_jax_jit(x_jax) \n",
    "print('Gaussian evaluation time with JAX-JIT (GPU): ', end='')\n",
    "%timeit gaussian_jax_jit(x_jax).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d53eec-9c15-4057-a845-9cdf4068245c",
   "metadata": {},
   "source": [
    "**S.1.3 Vectorization**\n",
    "\n",
    "An other optimization that JAX proposes is vectorization, which enables running multiple operations at the same time, most of the time in parallel. This is particularly useful when using GPUs since data transfers are expensive. In JAX, to transform a function into a vectorized version, we can just call `jax.vmap` on the corresponding function. By default, it operates on the first axis of the given function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af580a-a8cd-4030-a6a9-5f8d10e3f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gaussian_jax as if it was limited to scalar arguments\n",
    "def naive_batch_gaussian(x):\n",
    "  return jnp.stack([gaussian_jax(x_i) for x_i in x])\n",
    "\n",
    "N = 500\n",
    "x = jnp.linspace(-1.0, 1.0, 500)\n",
    "\n",
    "print('Naive batch with Python for loop (slow): ', end='')\n",
    "%timeit naive_batch_gaussian(x).block_until_ready()\n",
    "\n",
    "print('Batch with JAX vectorization: ', end='')\n",
    "%timeit jax.vmap(gaussian_jax)(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a874cb-e351-48e3-a0ce-8c4e7ddf1dfa",
   "metadata": {},
   "source": [
    "**S.1.4 Automatic differentiation (AD)**\n",
    "\n",
    "Finally, and most importantly, we need automatic differentiation. In mathematical terms, taking derivatives of a multivariate function is equivalent to computing the gradient of the function. In JAX, to transform a function to obtain its gradient using AD, we can just call `jax.grad` on the corresponding function. Note that `jax.grad` only support scalar-valued functions.\n",
    "\n",
    "---\n",
    "\n",
    "> *Exercise 2* : Run the following code section, then evaluate the gradiant of `gaussian_jax` at each point $x$ using `jax.vmap` vectorization.\n",
    "\n",
    "> *Exercise 3* : Compare the result of automatic differentiation with the exact Gaussian derivative $\\frac{d}{dx} (\\exp(-x^2)) = -2x \\exp(-x^2)$ and the result from numerical differentiation with `jnp.gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4ce43-d151-4877-8d15-4be5b36bc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_gaussian(x):\n",
    "  return jnp.sum(jnp.exp(-x**2))\n",
    "\n",
    "x = jnp.linspace(-1.0, 1.0, 4)\n",
    "print('Sum of the gaussians:',sum_gaussian(x))\n",
    "# Transform the function with automatic differentiation\n",
    "sum_gaussian_grad = jax.grad(sum_gaussian)\n",
    "print('Gradient of the sum of gaussians with respect to x:',sum_gaussian_grad(x))\n",
    "\n",
    "# Exercise 2\n",
    "x = jnp.linspace(-1.0, 1.0, 10, endpoint=False)\n",
    "derivative_gaussian_ad = jax.vmap(jax.grad(gaussian_jax))\n",
    "\n",
    "def gaussian_derivative(x):\n",
    "  return -2*x*jnp.exp(-x**2)\n",
    "\n",
    "# Exercise 3\n",
    "with jnp.printoptions(precision=2, suppress=True):\n",
    "  print('Automatic differentiation:',derivative_gaussian_ad(x))\n",
    "  print('Exact derivative         :',gaussian_derivative(x))\n",
    "  print('Numerical derivative     :',jnp.gradient(gaussian_jax(x), 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c113e5a0-bdd3-49b1-ba25-dab13daebdd9",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Reviews**\n",
    "\n",
    "* Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next, *Salvatore Cuomo, Vincenzo Schiano Di Cola, Fabio Giampaolo, Gianluigi Rozza, Maziar Raissi & Francesco Piccialli, Journal of Scientific Computing, 92(3), 88 (2022)*\n",
    "* Physics-informed neural networks (PINNs) for fluid mechanics: a review, *Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin & George Em Karniadakis, Acta Mechanica Sinica, 37(12), 1727-1738 (2022)*\n",
    "\n",
    "**Geophysics applications**\n",
    "\n",
    "* Physics-Informed Neural Networks (PINNs) for Wave Propagation and Full Waveform Inversions, *Majid Rasht-Behesht, Christian Huber, Khemraj Shukla, George Em Karniadakis, Journal of Geophysical Research: Solid Earth, 127(5), e2021JB023120 (2022)*\n",
    "* Joint Inversion of Geophysical Data for Geologic Carbon Sequestration Monitoring: A Differentiable Physics-Informed Neural Network Model, *Mingliang Liu, Divakar Vashisth, Dario Grana, Tapan Mukerji, Journal of Geophysical Research: Solid Earth, 128(3), e2022JB025372 (2023)*\n",
    "* Physics-informed neural networks for high-speed flows, *Zhiping Mao, Ameya D. Jagtap, George Em Karniadakis, Computer Methods in Applied Mechanics and Engineering, 360, 112789 (2020)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b8e16-26f0-4cb2-8e90-766dbbafb6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
